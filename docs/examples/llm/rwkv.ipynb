{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f5712-7a69-4e8f-a86a-95a63645e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.llms.rwkv import RWKVModel\n",
    "from llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a30091-8b38-4035-91a8-2fec8308bee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path /Users/emanuel/Library/Caches/llama_index/models/RWKV-4-World-0.4B-v1-20230529-ctx4096.pth\n",
      "model_url https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main/RWKV-4-World-0.4B-v1-20230529-ctx4096.pth\n",
      "model_name RWKV-4-World-0.4B-v1-20230529-ctx4096.pth\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 0\n",
      "\n",
      "Loading /Users/emanuel/Library/Caches/llama_index/models/RWKV-4-World-0.4B-v1-20230529-ctx4096.pth ...\n",
      "Strategy: (total 24+1=25 layers)\n",
      "* cpu [float32, float32], store 25 layers\n",
      "0-cpu-float32-float32 1-cpu-float32-float32 2-cpu-float32-float32 3-cpu-float32-float32 4-cpu-float32-float32 5-cpu-float32-float32 6-cpu-float32-float32 7-cpu-float32-float32 8-cpu-float32-float32 9-cpu-float32-float32 10-cpu-float32-float32 11-cpu-float32-float32 12-cpu-float32-float32 13-cpu-float32-float32 14-cpu-float32-float32 15-cpu-float32-float32 16-cpu-float32-float32 17-cpu-float32-float32 18-cpu-float32-float32 19-cpu-float32-float32 20-cpu-float32-float32 21-cpu-float32-float32 22-cpu-float32-float32 23-cpu-float32-float32 24-cpu-float32-float32 \n",
      "emb.weight                        f32      cpu  65536  1024 \n",
      "blocks.0.ln1.weight               f32      cpu   1024       \n",
      "blocks.0.ln1.bias                 f32      cpu   1024       \n",
      "blocks.0.ln2.weight               f32      cpu   1024       \n",
      "blocks.0.ln2.bias                 f32      cpu   1024       \n",
      "blocks.0.att.time_decay           f32      cpu   1024       \n",
      "blocks.0.att.time_first           f32      cpu   1024       \n",
      "blocks.0.att.time_mix_k           f32      cpu   1024       \n",
      "blocks.0.att.time_mix_v           f32      cpu   1024       \n",
      "blocks.0.att.time_mix_r           f32      cpu   1024       \n",
      "blocks.0.att.key.weight           f32      cpu   1024  1024 \n",
      "blocks.0.att.value.weight         f32      cpu   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32      cpu   1024  1024 \n",
      "blocks.0.att.output.weight        f32      cpu   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32      cpu   1024       \n",
      "blocks.0.ffn.time_mix_r           f32      cpu   1024       \n",
      "blocks.0.ffn.key.weight           f32      cpu   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32      cpu   1024  1024 \n",
      "blocks.0.ffn.value.weight         f32      cpu   4096  1024 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.23.ln1.weight              f32      cpu   1024       \n",
      "blocks.23.ln1.bias                f32      cpu   1024       \n",
      "blocks.23.ln2.weight              f32      cpu   1024       \n",
      "blocks.23.ln2.bias                f32      cpu   1024       \n",
      "blocks.23.att.time_decay          f32      cpu   1024       \n",
      "blocks.23.att.time_first          f32      cpu   1024       \n",
      "blocks.23.att.time_mix_k          f32      cpu   1024       \n",
      "blocks.23.att.time_mix_v          f32      cpu   1024       \n",
      "blocks.23.att.time_mix_r          f32      cpu   1024       \n",
      "blocks.23.att.key.weight          f32      cpu   1024  1024 \n",
      "blocks.23.att.value.weight        f32      cpu   1024  1024 \n",
      "blocks.23.att.receptance.weight   f32      cpu   1024  1024 \n",
      "blocks.23.att.output.weight       f32      cpu   1024  1024 \n",
      "blocks.23.ffn.time_mix_k          f32      cpu   1024       \n",
      "blocks.23.ffn.time_mix_r          f32      cpu   1024       \n",
      "blocks.23.ffn.key.weight          f32      cpu   1024  4096 \n",
      "blocks.23.ffn.receptance.weight   f32      cpu   1024  1024 \n",
      "blocks.23.ffn.value.weight        f32      cpu   4096  1024 \n",
      "ln_out.weight                     f32      cpu   1024       \n",
      "ln_out.bias                       f32      cpu   1024       \n",
      "head.weight                       f32      cpu   1024 65536 \n"
     ]
    }
   ],
   "source": [
    "llm = RWKVModel(\n",
    "    model_url=\"https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main/RWKV-4-World-0.4B-v1-20230529-ctx4096.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01965bdc-5565-47a2-b665-f6be28e4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window = 1536\n",
    "num_output = 300\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, context_window=context_window, num_output=num_output\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham/\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1f4f5-21c4-4bb8-b510-6e3d2ac69d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(top_k=5)\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf3a13-555c-4877-bfff-83d21385ff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "℅ [a] (as the same letter as the verb ℹ)\n",
      "\n",
      "(Yields:\n",
      "ℰA (first letter of a set) (a set of words)\n",
      "The answer to this question is (a set of words).\n",
      "----------------------\n",
      "\n",
      "I've used ~~ and ♛ to do this for quite a while. Here's a one-liner, but it would probably need to be modified to be more idiomatic:\n",
      "[m|p|l]m, (a set of words) [a set of words]\n",
      "and ♛, the other set of letters I use.\n",
      "\n",
      "It turns out that I'd need to have the context right for the first word to work. But my own understanding is that in the absence of context, it's just a matter of figuring out what part is supposed to do what, and which part does. It might be that you can't say ♛ for the context because you\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
